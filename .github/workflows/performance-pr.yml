name: Performance Comparison for Pull Requests

on:
  pull_request:
    branches: [master]

jobs:
  benchmark-pr:
    name: Performance benchmark comparison
    runs-on: ubuntu-latest
    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt -r requirements_dev.txt

      # Save commit SHAs for display
      - name: Save commit info
        id: commits
        run: |
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          HEAD_SHA="${{ github.event.pull_request.head.sha }}"
          echo "base_short=${BASE_SHA:0:7}" >> $GITHUB_OUTPUT
          echo "head_short=${HEAD_SHA:0:7}" >> $GITHUB_OUTPUT

      # Run benchmark on PR branch
      - name: Run benchmark on PR branch
        run: |
          pytest -o "python_files=benchmark_*.py" tests/benchmarks --benchmark-json=pr.json

      # Checkout base branch and run benchmark
      - name: Checkout base branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.base.sha }}
          clean: false
          path: base

      - name: Run benchmark on base branch
        working-directory: base
        run: |
          pip install -r requirements.txt -r requirements_dev.txt
          pytest -o "python_files=benchmark_*.py" tests/benchmarks --benchmark-json=../base.json

      # Compare benchmarks and generate report
      - name: Compare benchmarks
        id: compare
        run: |
          cat > comparison.md << 'EOF'
          ## Benchmark Comparison
          
          Comparing base branch (`${{ steps.commits.outputs.base_short }}`)
          vs PR branch (`${{ steps.commits.outputs.head_short }}`)
          
          ```
          EOF
          
          python3 - <<'PY' >> comparison.md
          import json
          import math
          import re
          
          def load_json(path):
              try:
                  with open(path) as f:
                      return json.load(f)
              except:
                  return None

          def format_val(val):
              if val is None: return "N/A"
              if val < 1e-9: return f"{val*1e9:.3f}n"
              if val < 1e-6: return f"{val*1e9:.3f}n"
              if val < 1e-3: return f"{val*1e6:.3f}¬µ"
              if val < 1: return f"{val*1e3:.3f}m"
              return f"{val:.3f}s"
          
          def format_stddev(val, mean):
              if val is None or mean == 0: return "¬± ?"
              return "¬± ‚àû ¬π" 

          def get_icon(diff_val):
              if diff_val > 10: return "üêå"
              if diff_val < -10: return "üöÄ"
              return "‚û°Ô∏è"

          def normalize_name(name):
              # Remove test_benchmark_ prefix
              name = re.sub(r'^test_benchmark_', '', name)
              # Convert snake_case to CamelCase
              parts = name.split('_')
              new_parts = []
              for p in parts:
                  if p.lower() in ['rbac', 'abac', 'acl', 'api', 'rest']:
                      new_parts.append(p.upper())
                  else:
                      new_parts.append(p.capitalize())
              return "".join(new_parts)

          base_data = load_json("base.json")
          pr_data = load_json("pr.json")
          
          if not base_data or not pr_data:
              print("Error: Could not load benchmark data.")
              exit(0)

          base_map = {b['name']: b['stats'] for b in base_data['benchmarks']}
          pr_map = {b['name']: b['stats'] for b in pr_data['benchmarks']}
          
          all_names = sorted(set(base_map.keys()) | set(pr_map.keys()))
          
          w_name = 50
          w_val = 18
          
          print("goos: linux")
          print("goarch: amd64")
          print("pkg: github.com/casbin/pycasbin")
          print("cpu: GitHub Actions Runner")
          
          # Headers aligned to match user request
          # ‚îÇ   base-bench.json   ‚îÇ        pr-bench.json        ‚îÇ
          # ‚îÇ       sec/op        ‚îÇ   sec/op       vs base      ‚îÇ   Delta
          
          header1 = f"{'':<{w_name}}‚îÇ   base-bench.json   ‚îÇ        pr-bench.json        ‚îÇ"
          header2 = f"{'':<{w_name}}‚îÇ       sec/op        ‚îÇ   sec/op       vs base      ‚îÇ   Delta"
          
          print(header1)
          print(header2)
          
          ratios = []
          base_means = []
          pr_means = []
          
          for name in all_names:
              base = base_map.get(name)
              pr = pr_map.get(name)
              
              base_mean = base['mean'] if base else 0
              pr_mean = pr['mean'] if pr else 0
              
              if base_mean > 0: base_means.append(base_mean)
              if pr_mean > 0: pr_means.append(pr_mean)
              
              base_str = f"{format_val(base_mean)} {format_stddev(base['stddev'] if base else 0, base_mean)}"
              pr_str = f"{format_val(pr_mean)} {format_stddev(pr['stddev'] if pr else 0, pr_mean)}"
              
              stats_placeholder = "~ (p=1.000 n=1) ¬≤"
              
              diff_str = ""
              icon = ""
              
              if base_mean != 0 and pr_mean != 0:
                  diff = (pr_mean - base_mean) / base_mean * 100
                  icon = get_icon(diff)
                  diff_str = f"{diff:+.2f}%"
                  ratios.append(pr_mean / base_mean)
              elif base_mean == 0 and pr_mean == 0:
                  diff_str = "0.00%"
                  icon = "‚û°Ô∏è"
                  ratios.append(1.0)
              
              display_name = normalize_name(name)
              
              line = f"{display_name:<{w_name}} {base_str:<{w_val}} {pr_str:<{w_val}} {stats_placeholder:<20} {diff_str:>7} {icon}"
              print(line)
              
          if ratios:
              if base_means and pr_means:
                  g_base = math.exp(sum(math.log(x) for x in base_means) / len(base_means))
                  g_pr = math.exp(sum(math.log(x) for x in pr_means) / len(pr_means))
                  
                  geo_diff = (g_pr - g_base) / g_base * 100
                  
                  g_base_str = format_val(g_base)
                  g_pr_str = format_val(g_pr)
                  
                  print(f"{'geomean':<{w_name}} {g_base_str:<{w_val}} {g_pr_str:<{w_val}} {'':<20} {geo_diff:>7.2f}%")
          
          print("¬π need >= 6 samples for confidence interval at level 0.95")
          print("¬≤ need >= 4 samples to detect a difference at alpha level 0.05")

          PY
          
          echo '```' >> comparison.md
          
        continue-on-error: true

      # Save PR number
      - name: Save PR number
        run: echo ${{ github.event.number }} > pr_number.txt

      # Upload benchmark results
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            comparison.md
            pr_number.txt
